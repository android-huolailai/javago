
# MQ
## 优点：
- 解耦：
发布订阅模式 -> BCD系统去 pull or push
- 异步: 写库操作。mq消费写库，直接返回客户端
- 削峰：每个时间点 -> 大量的消息 -> mysql 的写 2000QPS, 每秒请求 5000，可以将 请求封装 -> mq -> 慢慢的消费

## 缺点

1. 系统复杂度升高 -> 增加了复杂度
2. 系统可用性降低 -> mq挂了
## 问题
1. 重复消费的问题 幂等性
2. 消息堆积的问题
3. 消息顺序问题
4. 高可用问题
5. 消息丢失问题

# redis缓存
## 优点
- 高性能
对于复杂的查询操作 -> 缓存
- 高并发
mysql 这种数据 涉及 IO 操作 -> 不能支持高并发的查询，可以支持 几万 的 QPS
## 问题
1. 缓存一致性的问题
2. 高可用的问题
3. 缓存穿透的问题 缓存雪崩 缓存并发竞争 

## 对比 memcached
1. string 2. 集群 

## 线程模型
`file event handler` 文件事件处理器，单线程，IO多路复用机制监听多个 socket 将产生事件的socket压入队列，事件分派器根据事件类型选择相应的处理器进行处理。
- 多个socket
- IO 多路复用
- 事件处理器

redis与客户端进行一次通信的过程

![redis-single-thread-model](./images/redis-single-thread-model.png)

客户端 socket01 向 redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 `AE_READABLE` 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给**连接应答处理器**。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 `AE_READABLE` 事件与命令请求处理器关联。

假设此时客户端发送了一个 `set key value` 请求，此时 redis 中的 socket01 会产生 `AE_READABLE` 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 `AE_READABLE` 事件，由于前面 socket01 的 `AE_READABLE` 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 `key value` 并在自己内存中完成 `key value` 的设置。操作完成后，它会将 socket01 的 `AE_WRITABLE` 事件与命令回复处理器关联。

如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个 `AE_WRITABLE` 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 `ok`，之后解除 socket01 的 `AE_WRITABLE` 事件与命令回复处理器的关联。

## 为啥 redis 单线程模型也能效率这么高

- 纯内存操作。
- 核心是基于非阻塞的 IO 多路复用机制。
- C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。
- 单线程反而避免了多线程的频繁上下文切换问题，预防了多线程可能产生的竞争问题。
## 哨兵机制

哨兵机制实现了 redis 集群的高可用性。
- 集群监控： 监控redis 节点是否正常工作
- 消息通知： redis 节点挂了，通知管理员
- 选举新的master
- master挂了，通知client

判断一个 master node 挂了，是需要大多数的哨兵同意，涉及选举的问题。哨兵是集群的，至少 3个实例。
`哨兵` +`redis主从集群`只能保证集群的高可用，并不能保证数据不丢失。

## 数据情况丢失的问题

1. redis 是异步复制，master -> slave ，master挂了，丢失数据
2. `脑裂` 某个 master 网络不好，导致 哨兵以为它 挂了，重新进行选举 -> client 没来得及切换数据 -> `继续向旧的master写数据` -> 旧的master 数据被清空 -> 挂到新的 master上作为 slave,后来client写的数据就丢失了

## 数据丢失问题的解决方案

1. 至少一个 slave,数据复制 不超过 10秒，如果都超过10秒，master就不接受任何请求了
2. 减少数据同步的复制
3. 脑裂情况下还是会丢失数据，只不过丢失的是10秒的

## slave -> master 选举算法（待看）

## 生产环境
1. 集群的搭建由 运维部门基础架构部门负责（这个自己搭建的概率比较低）
2. 每个节点 5万 QPS
3. 10G 分给 redis
4. 32G+8CPU+300G

## 持久化

- RDB: 周期性持久化，冷备份，完成的文件上传云，恢复更快，fork子进程，进行 RDB持久化。
- AOF： `append-only`,数据损失小，在redis.conf中有相关的配置。append 数据文件大，支持压缩。具体参考 redis 持久化文章

## 主从架构
一主多从，主写从读。支持水平扩容，支持读高并发。
redis 最好开启 持久化，不能依靠 redis的从做热备，因为一旦 master挂掉，有可能清空 slave的
### redis 主从复制的原理

如果这是 slave node 初次连接到 master node，那么会触发一次 `full resynchronization` 全量复制。此时 master 会启动一个后台线程，开始生成一份 `RDB` 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中。`RDB` 文件生成完毕后， master 会将这个 `RDB` 发送给 slave，slave 会先**写入本地磁盘，然后再从本地磁盘加载到内存**中，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。

### 主从复制的断点续传
会记录log,offset

### slave 过期key的处理

slave 不会过期 key，只会等待 master 过期 key。如果 master 过期了一个 key，或者通过 LRU 淘汰了一个 key，那么会模拟一条 del 命令发送给 slave。

## 复制的完整流程
slave node 启动时，会在自己本地保存 master node 的信息，包括 master node 的`host`和`ip`，但是复制流程没开始。

slave node 内部有个定时任务，每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接。然后 slave node 发送 `ping` 命令给 master node。如果 master 设置了 requirepass，那么 slave node 必须发送 masterauth 的口令过去进行认证。master node **第一次执行全量复制**，将所有数据发给 slave node。而在后续，master node 持续将写命令，`异步`复制给 slave node。

![redis-master-slave-replication-detail](./images/redis-master-slave-replication-detail.png)

### heartbeat
主从节点互相都会发送 heartbeat 信息。

master 默认每隔 10秒 发送一次 heartbeat，slave node 每隔 1秒 发送一个 heartbeat。

## 更新策略
- 读：读缓存 -> 读数据库 -> 放入缓存 -> 返回响应
- 写：更新数据库，淘汰缓存（对于比较复杂的缓存可能是计算出来的）下次读取的时候 提到缓存 -> 更多 读少的情况。


# 数据库

## 分库分表
32个库，每个库 32个表，共 1024张表。 每个库 并发写1000，32个库就是 32000，如果是1500并发- >48000写并发，对于 8万的写 -> MQ -> db

1024 -> 每张表放 500万数据，可以放 50亿条数据。

| orderId | id % 32 (库) | id / 32 % 32 (表) | 计算出 表id

分库分表之后 id生成的问题 -> snowflake -> 改进算法 -> 美团的leaf,集群编号，时间戳。

数据迁移方案： 双写两个库 -> check -> 补齐

良好的单库 并发在 1000左右。

分库分表中间件 ： Sharding-jdbc, MyCat

水平分表 -> 表结构相同
垂直分表 -> 将不经常查询的列分出去 -> 缓存中可以容纳更多的热点数据。

## 设计高并发的系统
秀出全部的技能

- 系统拆分
- 缓存
- mq
- 读写分离
- 分表分库


# MQ

RabbitMQ的高可用性 -> 镜像集群 -> 配置 -> 每个都有一份 元数据 + 数据 -> 网络开销大 -> 不是分布式


---


## redis
#### redis 分布式锁


```r
set_resource_name my_random_value NX PX 3000 
```

- `NX`：表示只有 `key` 不存在的时候才会设置成功。（如果此时 redis 中存在这个 key，那么设置失败，返回 `nil`）
- `PX 30000`：意思是 30s 后锁自动释放。别人创建的时候如果发现已经有了就不能加锁了。

释放锁就是删除 key ，但是一般可以用 `lua` 脚本删除，判断 value 一样才删除：

```lua
-- 删除锁的时候，找到 key 对应的 value，跟自己传过去的 value 做比较，如果是一样的才删除。
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

为啥要用 `random_value` 随机值呢？因为如果某个客户端获取到了锁，但是阻塞了很长时间才执行完，比如说超过了 30s，此时可能已经自动释放锁了，此时可能别的客户端已经获取到了这个锁，要是你这个时候直接删除 key 的话会有问题，所以得用随机值加上面的 `lua` 脚本来释放锁。

但是这样是肯定不行的。因为如果是普通的 redis 单实例，那就是单点故障。或者是 redis 普通主从，那 redis 主从异步复制，如果主节点挂了（key 就没有了），key 还没同步到从节点，此时从节点切换为主节点，别人就可以 set key，从而拿到锁。

#### RedLock 算法
这个场景是假设有一个 redis cluster，有 5 个 redis master 实例。然后执行如下步骤获取一把锁：

1. 获取当前时间戳，单位是毫秒；
2. 跟上面类似，轮流尝试在每个 master 节点上创建锁，过期时间较短，一般就几十毫秒；
3. 尝试在**大多数节点**上建立一个锁，比如 5 个节点就要求是 3 个节点 `n / 2 + 1`；
4. 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了；
5. 要是锁建立失败了，那么就依次之前建立过的锁删除；
6. 只要别人建立了一把分布式锁，你就得**不断轮询去尝试获取锁**。

## ZK 分布式锁 

创建临时顺序节点：

如果有一把锁，被多个人给竞争，此时多个人会排队，第一个拿到锁的人会执行，然后释放锁；后面的每个人都会去监听**排在自己前面**的那个人创建的 node 上，一旦某个人释放了锁，排在自己后面的人就会被 zookeeper 给通知，一旦被通知了之后，就 ok 了，自己就获取到了锁，就可以执行代码了。

- redis 分布式锁，其实**需要自己不断去尝试获取锁**，比较消耗性能。
- zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。


## 幂等性

分布式接口的幂等性 。orderId (唯一性) + redis流水

## 请求的顺序性

严格的顺序会 导致系统变慢，复杂度提升， 保证 hash 最终分配到同一台集器，然后内存排队。

## 分布式事务

1. XA : 两阶段提交 ，事务管理器 协调事务。A,B,C好了么，Ok -> 提交
2. 三阶段提交 try confirm cancle
### 可靠消息的一致性方案

